{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pystac\n",
    "from stac2webdav.utils import catalog2geopandas\n",
    "\n",
    "\n",
    "def read_tile_catalog(catalog_path):\n",
    "    \"\"\" Read tile catalog \"\"\"\n",
    "    catalog_path = pathlib.Path(catalog_path)\n",
    "    catalog_path = catalog_path / \"catalog.json\"\n",
    "    return pystac.Catalog.from_file(catalog_path.as_posix())\n",
    "\n",
    "\n",
    "def read_labels(labels_path):\n",
    "    \"\"\" Read all labels, and merge them in a single GeoDataFrame \"\"\"\n",
    "    labels_path = pathlib.Path(labels_path)\n",
    "    labels = [gpd.read_file(p) for p in labels_path.glob(\"*.geojson\")]\n",
    "    crs = labels[0].crs\n",
    "    assert all([l.crs == crs for l in labels])\n",
    "    labels = pd.concat(labels).pipe(gpd.GeoDataFrame)\n",
    "    return labels.set_crs(crs)\n",
    "\n",
    "\n",
    "def get_asset_paths(catalog, item_ids, asset_key):\n",
    "    \"\"\" Extract the asset paths from the catalog \"\"\"\n",
    "    items = (catalog.get_item(id, recursive=True) for id in item_ids)\n",
    "    assets = (item.assets[asset_key] for item in items)\n",
    "    return [asset.get_absolute_href() for asset in assets]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input configurations\n",
    "catalog_path = \"./S2_composite_catalog\"\n",
    "labels_path = \"./labels\"\n",
    "test_set_size = 12  # num of tiles in the test set\n",
    "validation_split = 0.3  # fraction of the remaining data for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tile catalog\n",
    "catalog = read_tile_catalog(catalog_path)\n",
    "tiles = catalog2geopandas(catalog)\n",
    "\n",
    "# read labels \n",
    "labels = read_labels(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the only labels matching the tiles timespan\n",
    "label_dates = pd.to_datetime(labels.Date)\n",
    "start_datetime = pd.to_datetime(tiles.start_datetime).min() \n",
    "end_datetime = pd.to_datetime(tiles.end_datetime).max() \n",
    "mask = (label_dates >= start_datetime) & (label_dates <= end_datetime)\n",
    "labels = labels[mask]\n",
    "\n",
    "# reserve the labeled tiles for the test set\n",
    "mask = tiles.intersects(labels.unary_union)\n",
    "test_set_labeled = tiles[mask]\n",
    "\n",
    "# pick additional unlabeled tiles for the test set\n",
    "n_tiles_labeled = len(test_set_labeled)\n",
    "n_tiles_unlabeled = test_set_size - n_tiles_labeled\n",
    "test_set_unlabeled = tiles[~mask].sample(n_tiles_unlabeled)\n",
    "\n",
    "# split test set and training/validation set\n",
    "test_set = pd.concat([test_set_labeled, test_set_unlabeled])\n",
    "train_set = tiles.index.difference(test_set.index)\n",
    "train_set = tiles.loc[train_set]\n",
    "\n",
    "# number of tiles in the test set from labeled/unlabeled data\n",
    "n_tiles_labeled, n_tiles_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set and validation set\n",
    "val_set_size = round(validation_split*len(train_set))\n",
    "val_set = train_set.sample(val_set_size)\n",
    "train_set = train_set.index.difference(val_set.index)\n",
    "train_set = tiles.loc[train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tile paths from the catalog\n",
    "test_set_paths = get_asset_paths(catalog, test_set.index, \"B2-B3-B4-B11\")\n",
    "train_set_paths = get_asset_paths(catalog, train_set.index, \"B2-B3-B4-B11\")\n",
    "val_set_paths = get_asset_paths(catalog, val_set.index, \"B2-B3-B4-B11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = gpd.read_file(\"./ne_10m_antarctic_ice_shelves_polys/ne_10m_antarctic_ice_shelves_polys.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no balancing in the test set (i.e. don't apply mask)\n",
    "test_set = Dataset(test_set_paths, sizeCutOut, bands, shuffle_tiles=True)\n",
    "test_set_tf = test_set.to_tf()\n",
    "\n",
    "# balanced validation set (i.e. apply mask)\n",
    "val_set = Dataset(val_set_paths, sizeCutOut, bands, shuffle_tiles=True)\n",
    "val_set.set_mask(mask.unary_union, crs=mask.crs)\n",
    "val_set_tf = val_set.to_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_set_tf = val_set_tf.shuffle(buffer) # if we use a subset, we should probably shuffle this\n",
    "val_set_tf = val_set_tf.batch(64, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_tf = test_set_tf.shuffle(buffer) # if we use a subset, we should probably shuffle this\n",
    "test_set_tf = test_set_tf.batch(64, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while epochcounter < nEpochsMax:\n",
    "    ...\n",
    "    train_set = Dataset(train_set_paths, sizeCutOut, offset=offset,\n",
    "                        shuffle_tiles=True)\n",
    "    train_set.set_mask(mask.unary_union, crs=mask.crs)\n",
    "    train_set_tf = train_set.to_tf()\n",
    "    train_set_tf = train_set_tf.shuffle(buffersize=3000000).batch(64, drop_remainder=True)\n",
    "    ...\n",
    "    model.fit(\n",
    "        ...\n",
    "        validation_data=val_set_tf,\n",
    "        # validation_steps=100000  # use a subset of the validation set (it will be the same for all epochs) \n",
    "        ...\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
